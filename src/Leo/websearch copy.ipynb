{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured titles have been saved to /Users/giantleo/Desktop/AICGU/structured_titles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from selenium.common.exceptions import StaleElementReferenceException, WebDriverException\n",
    "\n",
    "# Initialize Selenium\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def is_cgu_link(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return 'cgu.edu' in parsed_url.netloc\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for JavaScript to load\n",
    "        content = ' '.join([element.text for element in driver.find_elements(By.TAG_NAME, 'p')])\n",
    "        if not content:\n",
    "            raise ValueError(\"No content found; check JavaScript or AJAX calls.\")\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def generate_title_and_subtitle_from_ai(url, api_key, existing_title=\"\"):\n",
    "    try:\n",
    "        content = extract_text_from_url(url)\n",
    "        \n",
    "        # Generate main title\n",
    "        prompt_main_title = f\"Generate a concise and accurate main title for the following webpage content:\\n\\n{content}\"\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "        response_main = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of generating accurate main titles.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_main_title}\n",
    "            ],\n",
    "            max_tokens=10,  # Shorter output for main title\n",
    "            temperature=0.7\n",
    "        )\n",
    "        main_title = response_main.choices[0].message.content.strip()\n",
    "\n",
    "        # Generate subtitle only if it's not a main title page\n",
    "        prompt_subtitle = f\"Generate an optional subtitle for the following main title and webpage content if it relates to a specific category or subcategory under the main title:\\n\\nMain Title: {main_title}\\n\\n{content}\"\n",
    "        response_sub = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of generating relevant subtitles.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_subtitle}\n",
    "            ],\n",
    "            max_tokens=100,  # Shorter output for subtitle\n",
    "            temperature=0.2\n",
    "        )\n",
    "        subtitle = response_sub.choices[0].message.content.strip()\n",
    "\n",
    "        # Ensure subtitle is meaningful\n",
    "        if subtitle.lower() in [\"none\", \"no subtitle\", \"\"]:\n",
    "            subtitle = \"\"\n",
    "\n",
    "        return main_title, subtitle\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating title for {url} with AI: {e}\")\n",
    "        return existing_title, \"\"\n",
    "\n",
    "def get_webpage_links(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    links = []\n",
    "    for element in elements:\n",
    "        try:\n",
    "            href = element.get_attribute('href')\n",
    "            title = element.text.strip()\n",
    "            if href and href.startswith('http') and is_cgu_link(href):\n",
    "                links.append({'title': title, 'url': href})\n",
    "        except StaleElementReferenceException:\n",
    "            continue\n",
    "    return links\n",
    "\n",
    "def get_all_webpage_links(url, max_links=100):\n",
    "    visited = set()\n",
    "    all_links = []\n",
    "\n",
    "    def _get_webpage_links(url, depth=1, max_depth=2):\n",
    "        if depth > max_depth or len(all_links) >= max_links:\n",
    "            return\n",
    "        links = get_webpage_links(url)\n",
    "        for link in links:\n",
    "            href = link['url']\n",
    "            if href not in visited:\n",
    "                visited.add(href)\n",
    "                all_links.append(link)\n",
    "                if len(all_links) >= max_links:\n",
    "                    break\n",
    "                _get_webpage_links(href, depth + 1, max_depth)\n",
    "        return\n",
    "\n",
    "    _get_webpage_links(url)\n",
    "    return all_links\n",
    "\n",
    "def ensure_titles(links, api_key):\n",
    "    main_titles = []\n",
    "    for link in links:\n",
    "        if not link['title'] or len(link['title']) < 5:  # Assuming titles less than 5 characters are not useful\n",
    "            main_title, subtitle = generate_title_and_subtitle_from_ai(link['url'], api_key, link['title'])\n",
    "            link['main_title'] = main_title\n",
    "            link['subtitle'] = subtitle\n",
    "        else:\n",
    "            link['main_title'] = link['title']\n",
    "            link['subtitle'] = \"\"\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, filepath):\n",
    "    with open(filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Main Title', 'Subtitle', 'URL'])\n",
    "        for link in links:\n",
    "            writer.writerow([link['main_title'], link['subtitle'], link['url']])\n",
    "\n",
    "api_key = ''  # Replace with your actual API key\n",
    "main_url = 'https://www.cgu.edu'\n",
    "all_links = get_all_webpage_links(main_url, max_links=100)\n",
    "\n",
    "# Ensure all links have titles\n",
    "all_links = ensure_titles(all_links, api_key)\n",
    "\n",
    "# Save the links to a CSV file in the current file path\n",
    "current_path = os.getcwd()\n",
    "csv_file_path = os.path.join(current_path, 'structured_titles.csv')\n",
    "save_links_to_csv(all_links, csv_file_path)\n",
    "\n",
    "print(f\"Structured titles have been saved to {csv_file_path}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize Selenium WebDriver in headless mode to run without a GUI.\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def is_cgu_main_link(url):\n",
    "    \"\"\"\n",
    "    Check if the URL is from the main CGU website (not subdomains or external sites),\n",
    "    and also check that it does not lead to the news or events sections.\n",
    "    Args:\n",
    "        url (str): The URL to check.\n",
    "    Returns:\n",
    "        bool: True if the URL is from 'www.cgu.edu' and not part of excluded paths, False otherwise.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.netloc != 'www.cgu.edu':\n",
    "        return False\n",
    "    excluded_paths = ['/news/', '/events/', '/event/','/new/']  # Paths to exclude\n",
    "    return not any(excluded_path in parsed_url.path for excluded_path in excluded_paths)\n",
    "\n",
    "def get_webpage_links(url, depth=0, max_depth=20):\n",
    "    \"\"\"\n",
    "    Recursively fetch links from the given URL up to a specified depth, excluding specific paths.\n",
    "    Args:\n",
    "        url (str): The starting URL to fetch links from.\n",
    "        depth (int): The current depth of the recursion.\n",
    "        max_depth (int): The maximum depth to recurse.\n",
    "    Returns:\n",
    "        set: A set of unique URLs collected from the website.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    links = set()\n",
    "\n",
    "    def fetch_links(current_url, current_depth):\n",
    "        if current_depth > max_depth or current_url in visited:\n",
    "            return\n",
    "\n",
    "        visited.add(current_url)\n",
    "        driver.get(current_url)\n",
    "        time.sleep(2)  # Allow for page loading and JavaScript execution.\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "        for element in elements:\n",
    "            href = element.get_attribute('href')\n",
    "            if href and is_cgu_main_link(href) and href not in visited:\n",
    "                links.add(href)\n",
    "\n",
    "        # Recursively fetch links from newly discovered pages.\n",
    "        for link in list(links):\n",
    "            if is_cgu_main_link(link):  # Check link again before recursive fetch\n",
    "                fetch_links(link, current_depth + 1)\n",
    "\n",
    "    fetch_links(url, depth)\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, filepath):\n",
    "    \"\"\"\n",
    "    Save the collected links to a CSV file.\n",
    "    Args:\n",
    "        links (set): A set of URLs to save.\n",
    "        filepath (str): The path to the CSV file where the links will be saved.\n",
    "    \"\"\"\n",
    "    with open(filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['URL'])  # Writing the header.\n",
    "        for link in links:\n",
    "            writer.writerow([link])\n",
    "\n",
    "# Starting URL from the CGU website.\n",
    "main_url = 'https://www.cgu.edu/school/center-for-information-systems-and-technology/faculty/'\n",
    "all_links = get_webpage_links(main_url)\n",
    "\n",
    "# Define the path for the CSV file to store the links.\n",
    "current_path = os.getcwd()\n",
    "csv_file_path = os.path.join(current_path, 'cgu_faculty.csv')\n",
    "save_links_to_csv(all_links, csv_file_path)\n",
    "\n",
    "print(f\"All links have been saved to {csv_file_path}\")\n",
    "\n",
    "driver.quit()  # Close the browser once done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for the CISAT Facutly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All links have been saved to /Users/giantleo/Desktop/AICGU/cgu_faculty_links.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize Selenium WebDriver in headless mode to run without a GUI.\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def get_webpage_links(url):\n",
    "    \"\"\"\n",
    "    Fetch all clickable links from the given URL.\n",
    "    Args:\n",
    "        url (str): The URL to fetch links from.\n",
    "    Returns:\n",
    "        set: A set of unique URLs collected from the webpage.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow for page loading and JavaScript execution.\n",
    "    elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    links = set(element.get_attribute('href') for element in elements if element.get_attribute('href'))\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, filepath):\n",
    "    \"\"\"\n",
    "    Save the collected links to a CSV file.\n",
    "    Args:\n",
    "        links (set): A set of URLs to save.\n",
    "        filepath (str): The path to the CSV file where the links will be saved.\n",
    "    \"\"\"\n",
    "    with open(filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['URL'])  # Writing the header.\n",
    "        for link in links:\n",
    "            writer.writerow([link])\n",
    "\n",
    "# Starting URL from the CGU website.\n",
    "main_url = 'https://www.cgu.edu/school/center-for-information-systems-and-technology/faculty/'\n",
    "all_links = get_webpage_links(main_url)\n",
    "\n",
    "# Define the path for the CSV file to store the links.\n",
    "current_path = os.getcwd()\n",
    "csv_file_path = os.path.join(current_path, 'cgu_faculty_links.csv')\n",
    "save_links_to_csv(all_links, csv_file_path)\n",
    "\n",
    "print(f\"All links have been saved to {csv_file_path}\")\n",
    "\n",
    "driver.quit()  # Close the browser once done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize Selenium WebDriver in headless mode to run without a GUI.\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def is_news_link(url):\n",
    "    \"\"\"\n",
    "    Check if the URL is from the CGU news section.\n",
    "    Args:\n",
    "        url (str): The URL to check.\n",
    "    Returns:\n",
    "        bool: True if the URL is a valid news link, False otherwise.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc == 'www.cgu.edu' and parsed_url.path.startswith('/news/')\n",
    "\n",
    "def get_webpage_links(url):\n",
    "    \"\"\"\n",
    "    Fetch links from the given URL, handling pagination.\n",
    "    Args:\n",
    "        url (str): The starting URL to fetch links from.\n",
    "    Returns:\n",
    "        set: A set of unique URLs collected from the website.\n",
    "    \"\"\"\n",
    "    links = set()\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow for page loading and JavaScript execution.\n",
    "\n",
    "    while True:\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        for element in elements:\n",
    "            href = element.get_attribute('href')\n",
    "            if href and is_news_link(href):\n",
    "                links.add(href)\n",
    "\n",
    "        # Find the next page button and click it if there is one\n",
    "        next_page_buttons = driver.find_elements(By.XPATH, \"//a[contains(text(), 'Next')]\")\n",
    "        if next_page_buttons:\n",
    "            next_page = next_page_buttons[-1]  # Usually the last 'Next' button is the correct one\n",
    "            if 'disabled' not in next_page.get_attribute('class'):\n",
    "                driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "                time.sleep(2)  # Wait for the page to load after clicking\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return links\n",
    "\n",
    "def save_links_to_csv(links, filepath):\n",
    "    \"\"\"\n",
    "    Save the collected links to a CSV file.\n",
    "    Args:\n",
    "        links (set): A set of URLs to save.\n",
    "        filepath (str): The path to the CSV file where the links will be saved.\n",
    "    \"\"\"\n",
    "    with open(filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['URL'])  # Writing the header.\n",
    "        for link in links:\n",
    "            writer.writerow([link])\n",
    "\n",
    "# Starting URL from the CGU news website.\n",
    "news_url = 'https://www.cgu.edu/news/'\n",
    "all_links = get_webpage_links(news_url)\n",
    "\n",
    "# Define the path for the CSV file to store the links.\n",
    "current_path = os.getcwd()\n",
    "csv_file_path = os.path.join(current_path, 'cgu_news_links.csv')\n",
    "save_links_to_csv(all_links, csv_file_path)\n",
    "\n",
    "print(f\"All news links have been saved to {csv_file_path}\")\n",
    "\n",
    "driver.quit()  # Close the browser once done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    URL\n",
      "0     https://www.cgu.edu/news/2021/09/passings-pame...\n",
      "1     https://www.cgu.edu/news/2020/12/ses-alumna-pl...\n",
      "3     https://www.cgu.edu/news/2021/04/financial-vir...\n",
      "4     https://www.cgu.edu/news/2017/08/elevating-edu...\n",
      "5     https://www.cgu.edu/news/2016/07/drucker-schoo...\n",
      "...                                                 ...\n",
      "1294  https://www.cgu.edu/news/2016/02/english-prof-...\n",
      "1295  https://www.cgu.edu/news/2013/11/drucker-schoo...\n",
      "1296  https://www.cgu.edu/news/2017/06/cgu-alumnus-l...\n",
      "1297  https://www.cgu.edu/news/2022/07/is-inflation-...\n",
      "1299  https://www.cgu.edu/news/2020/04/identifying-r...\n",
      "\n",
      "[933 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cgu_news_links.csv')\n",
    "\n",
    "# Filter out URLs ending with '#page', '#', '#header-search-form', or containing '/news/page/'\n",
    "filtered_data = data[\n",
    "    ~data['URL'].str.endswith(('#page', '#', '#header-search-form')) &  # Exclude URLs ending with specified patterns\n",
    "    ~data['URL'].str.contains('/news/page/')  # Exclude URLs containing '/news/page/'\n",
    "]\n",
    "\n",
    "# Save the filtered data to a new CSV file, if needed\n",
    "filtered_data.to_csv('filtered_cgu_news_links.csv', index=False)\n",
    "\n",
    "# Display the filtered data\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  URL\n",
      "0                          https://www.cgu.edu/apply/\n",
      "1                          https://www.cgu.edu/about/\n",
      "2          https://www.cgu.edu/people/warren-roberts/\n",
      "3         https://www.cgu.edu/the-claremont-colleges/\n",
      "4                                https://www.cgu.edu/\n",
      "..                                                ...\n",
      "78       https://www.cgu.edu/people/samir-chatterjee/\n",
      "79  https://www.cgu.edu/student-life/civil-rights-...\n",
      "80  https://www.cgu.edu/school/center-for-informat...\n",
      "82  https://www.cgu.edu/school/center-for-informat...\n",
      "83  https://www.cgu.edu/school/center-for-informat...\n",
      "\n",
      "[78 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('faculty_CISAT.csv')\n",
    "\n",
    "# Filter out URLs that do not start with 'https://'\n",
    "filtered_data = data[data['URL'].str.startswith('https://')]\n",
    "\n",
    "# Save the filtered data to a new CSV file, if needed\n",
    "filtered_data.to_csv('filtered_cgu_links_2.csv', index=False)\n",
    "\n",
    "# Display the filtered data\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   URL\n",
      "0    https://www.cgu.edu/news/2021/09/passings-pame...\n",
      "1    https://www.cgu.edu/news/2020/12/ses-alumna-pl...\n",
      "2    https://www.cgu.edu/news/2021/04/financial-vir...\n",
      "3    https://www.cgu.edu/news/2017/08/elevating-edu...\n",
      "4    https://www.cgu.edu/news/2016/07/drucker-schoo...\n",
      "..                                                 ...\n",
      "928  https://www.cgu.edu/news/2016/02/english-prof-...\n",
      "929  https://www.cgu.edu/news/2013/11/drucker-schoo...\n",
      "930  https://www.cgu.edu/news/2017/06/cgu-alumnus-l...\n",
      "931  https://www.cgu.edu/news/2022/07/is-inflation-...\n",
      "932  https://www.cgu.edu/news/2020/04/identifying-r...\n",
      "\n",
      "[933 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from both CSV files\n",
    "main_data = pd.read_csv('filtered_cgu_links_2.csv')\n",
    "cgu_links_data = pd.read_csv('filtered_cgu_news_links.csv')\n",
    "\n",
    "# Convert the URLs from the main CSV file into a set\n",
    "urls_in_main = set(main_data['URL'])\n",
    "\n",
    "# Filter out URLs in CGU_links that appear in the main CSV file\n",
    "filtered_cgu_links = cgu_links_data[~cgu_links_data['URL'].isin(urls_in_main)]\n",
    "\n",
    "# Save the filtered data to a new CSV file, if needed\n",
    "filtered_cgu_links.to_csv('filtered_cgu_news_links.csv', index=False)\n",
    "\n",
    "# Display the filtered data\n",
    "print(filtered_cgu_links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "websers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All content processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def fetch_and_process_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Attempt to capture the title from <h1> or fallback to <title>\n",
    "        title_tag = soup.find('h1')\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "        else:\n",
    "            title = soup.title.text.strip() if soup.title else 'No Title'\n",
    "\n",
    "        # Extract all meaningful text from common text-bearing elements\n",
    "        text_elements = soup.find_all(['p', 'span', 'div', 'li', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        text_content = ' '.join([elem.text.strip() for elem in text_elements if elem.text.strip()])\n",
    "\n",
    "        return title, text_content\n",
    "    except requests.RequestException as e:\n",
    "        return 'Error', str(e)\n",
    "\n",
    "def chunk_text(text, chunk_size=1024):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Load the CSV file containing URLs\n",
    "data = pd.read_csv('filtered_cgu_links_2.csv')\n",
    "output_dir = 'scraped_texts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Limit to the first 10 URLs for testing\n",
    "for index, row in data.iterrows():\n",
    "    if index >= 10:\n",
    "        break\n",
    "\n",
    "    url = row['URL']\n",
    "    title, content = fetch_and_process_content(url)\n",
    "\n",
    "    if content:  # Only process pages with content\n",
    "        chunks = chunk_text(content)\n",
    "        with open(os.path.join(output_dir, f\"{index}_content.txt\"), 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {title}\\n\\n\")\n",
    "            for chunk in chunks:\n",
    "                file.write(chunk + \"\\n\\n\")\n",
    "    else:\n",
    "        print(f\"No content found for URL: {url}\")\n",
    "\n",
    "print(\"All content processed and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
